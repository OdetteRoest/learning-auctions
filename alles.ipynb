{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Mechanism Components\n",
    "\n",
    "The dmch package contains data structures and algorithms for deep mechanism design.\n",
    "\n",
    "The package uses pytorch for underlying tensor operations.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "__author__       = 'Patrick R. Jordan'\n",
    "__email__        = 'patrick.r.jordan@gmail.com'\n",
    "__version__      = '0.1.0'\n",
    "__url__          = 'https://github.com/pjordan/dmch/',\n",
    "__description__  = 'Deep Mechanism Design Components'\n",
    "\n",
    "# import to_inputs, from_inputs\n",
    "# import Allocation, SequentialAllocation, Payment, Mechanism\n",
    "# import build_allocation_rule, build_payment_rule, build_mechanism, build_spa\n",
    "# import SequentialMechanism\n",
    "# import create_spa_mechanism\n",
    "# from .training import train, evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Builders \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear\n",
    "\n",
    "# import Allocation\n",
    "# import SequentialAllocation\n",
    "# import Payment\n",
    "# import Mechanism\n",
    "\n",
    "# from .spa import create_spa_allocator, create_spa_pricer\n",
    "# import create_monotonic\n",
    "# from .sequential import SequentialMechanism\n",
    "\n",
    "class _LeftAddition(Module):\n",
    "    def __init__(self, left_transformation):\n",
    "        super(_LeftAddition, self).__init__()\n",
    "        self.left_transformation = left_transformation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.left_transformation(x)+x\n",
    "    \n",
    "class _BaseBidResponseBuilder(object):\n",
    "    def __init__(self, bidders, context_features):\n",
    "        self.bidders = bidders\n",
    "        self.context_features = context_features\n",
    "        self.current_features = bidders + context_features\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_identity(self):\n",
    "        self.layers.append(nn.Identity())\n",
    "    \n",
    "    def add_batch_normalization(self):\n",
    "        self.layers.append(nn.BatchNorm1d(self.current_features))\n",
    "    \n",
    "    def add_batch_normalization(self):\n",
    "        self.layers.append(nn.BatchNorm1d(self.current_features))\n",
    "        \n",
    "    def add_sigmoid_activation(self):\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "        \n",
    "    def add_leaky_relu_activation(self):\n",
    "        self.layers.append(nn.LeakyReLU())\n",
    "    \n",
    "    def add_activation(self, act_module):\n",
    "        self.layers.append(act_module)\n",
    "        \n",
    "    def add_linear_layer(self, out_features):\n",
    "        self.layers.append(nn.Linear(self.current_features, out_features))\n",
    "        self.current_features = out_features\n",
    "        \n",
    "    def add_residual_layer(self, act_layer=nn.LeakyReLU):\n",
    "        self.layers.append(\n",
    "            _LeftAddition(\n",
    "                nn.Sequential(\n",
    "                    nn.BatchNorm1d(self.current_features),\n",
    "                    act_layer(),\n",
    "                    nn.Linear(self.current_features, self.current_features),\n",
    "                    nn.BatchNorm1d(self.current_features),\n",
    "                    act_layer(),\n",
    "                    nn.Linear(self.current_features, self.current_features)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def build(self):\n",
    "        pass\n",
    "    \n",
    "class _AllocationRuleBuilder(_BaseBidResponseBuilder):    \n",
    "    def build(self):\n",
    "        layers = self.layers + [Allocation(self.current_features, self.bidders)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def build_sequential(self,slots,weights=None):\n",
    "        layers = self.layers + [SequentialAllocation(self.current_features, slots, self.bidders, weights=weights)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "class _PaymentRuleBuilder(_BaseBidResponseBuilder):    \n",
    "    def build(self):\n",
    "        layers = self.layers + [Payment(self.current_features, self.bidders)]\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "class _MechanismBuilder(object):\n",
    "    def __init__(self, bidders, context_features=0):\n",
    "        self.allocation_builder = build_allocation_rule(bidders, context_features=context_features)\n",
    "        self.payment_builder = build_payment_rule(bidders, context_features=context_features)\n",
    "    \n",
    "    def build(self):\n",
    "        return Mechanism(\n",
    "            self.allocation_builder.build(),\n",
    "            self.payment_builder.build())\n",
    "    \n",
    "    def build_sequential(self,slots,weights=None):\n",
    "        return Mechanism(\n",
    "            self.allocation_builder.build_sequential(slots,weights=weights),\n",
    "            self.payment_builder.build())\n",
    "    \n",
    "class _SpaBuilder(object):\n",
    "    def __init__(self, bidders, context_features=0):\n",
    "        self.bidders = bidders\n",
    "        self.context_features = context_features\n",
    "        self.virtual_fcn = None\n",
    "        \n",
    "    def set_virtual_function(self, hidden_features=1, linear_functions=1, groups=1):\n",
    "        self.virtual_fcn = create_monotonic(\n",
    "            context_features=self.context_features,\n",
    "            hidden_features=hidden_features,\n",
    "            linear_functions=linear_functions,\n",
    "            groups=groups)\n",
    "\n",
    "    def build(self):\n",
    "        return create_spa_mechanism(self.bidders, context_features=self.context_features)\n",
    "    \n",
    "    def build_sequential(self, slots, weights=None):\n",
    "        return SequentialMechanism(\n",
    "            [create_spa_allocator(self.bidders) for _ in range(slots)],\n",
    "            [create_spa_pricer(self.bidders) for _ in range(slots)],\n",
    "            self.bidders,\n",
    "            weights=weights,\n",
    "            virtual_fcn=self.virtual_fcn\n",
    "        )\n",
    "    \n",
    "def build_allocation_rule(bidders, context_features=0):\n",
    "    return _AllocationRuleBuilder(bidders, context_features=context_features)\n",
    "\n",
    "def build_payment_rule(bidders, context_features=0):\n",
    "    return _PaymentRuleBuilder(bidders, context_features=context_features)\n",
    "\n",
    "def build_mechanism(bidders, context_features=0):\n",
    "    return _MechanismBuilder(bidders, context_features=context_features)\n",
    "\n",
    "def build_spa(bidders, context_features=0):\n",
    "    return _SpaBuilder(bidders, context_features=context_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def to_inputs(bids, context):\n",
    "    return torch.cat((bids,context), dim=1)\n",
    "\n",
    "def from_inputs(inputs, bidders):\n",
    "    return torch.split(inputs, (bidders,inputs.shape[1]-bidders), dim=1)\n",
    "    \n",
    "def utility(allocation, payment, values):\n",
    "    return allocation*values-payment\n",
    "\n",
    "def revenue(payment):\n",
    "    return payment.sum(dim=-1)\n",
    "\n",
    "def welfare(allocation, values):\n",
    "    return (allocation * values).sum(dim=-1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanism_modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear\n",
    "\n",
    "def _sequential_allocation(p, weights):\n",
    "    _, slots, bidders_plus_one = p.shape\n",
    "    bidders = bidders_plus_one - 1\n",
    "    \n",
    "    # total probability of allocating to slot 0\n",
    "    cumulative_total = p[:,0,:bidders]\n",
    "    \n",
    "    # weighted total allocation\n",
    "    if weights is None:\n",
    "        alloc = cumulative_total\n",
    "    else:\n",
    "        alloc = cumulative_total * weights[0]\n",
    "    \n",
    "    for k in range(1,slots):\n",
    "        # total probability of allocating to slot k\n",
    "        slot_total = (1-cumulative_total)*p[:,k,:bidders]*(1-p[:,k-1,[bidders for _ in range(bidders)]])\n",
    "\n",
    "        # weighted total allocation\n",
    "        if weights is None:\n",
    "            alloc = alloc + slot_total\n",
    "        else:\n",
    "            alloc = alloc + slot_total * weights[k]\n",
    "        \n",
    "        cumulative_total = cumulative_total + slot_total\n",
    "    return alloc\n",
    "    \n",
    "class Allocation(Module):\n",
    "    r\"\"\"Determines allocation probability for each of the bidders given an input.\n",
    "    \n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        bidders: number of bidders, which governs the size of each output sample\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
    "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
    "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{bidders}`.\n",
    "\n",
    "    Examples::\n",
    "        >>> m = Allocation(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> allocation = m(input)\n",
    "        >>> print(allocation.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "    __constants__ = ['in_features', 'bidders']\n",
    "    \n",
    "    def __init__(self, in_features, bidders):\n",
    "        super(Allocation, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.bidders = bidders\n",
    "        self.linear = Linear(in_features, bidders+1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.linear(x), dim=1)[:, 0:self.bidders]\n",
    "\n",
    "class SequentialAllocation(Module):\n",
    "    __constants__ = ['in_features', 'bidders', 'slots', 'weights']\n",
    "    \n",
    "    def __init__(self, in_features, slots, bidders, weights=None):\n",
    "        super(SequentialAllocation, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.slots = slots\n",
    "        self.bidders = bidders\n",
    "        self.weights = weights\n",
    "        self.linear = Linear(in_features, slots * (bidders+1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        probs = F.softmax(self.linear(x).reshape(-1, self.slots, self.bidders+1), dim=2)\n",
    "        return _sequential_allocation(probs,weights=self.weights)\n",
    "\n",
    "class Payment(Module):\n",
    "    r\"\"\"Determines the contingent payment for each of the bidders given an input.\n",
    "    \n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        bidders: number of bidders, which governs the size of each output sample\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
    "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
    "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{bidders}`.\n",
    "\n",
    "    Examples::\n",
    "        >>> m = Allocation(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> payment = m(input)\n",
    "        >>> print(payment.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, bidders):\n",
    "        super(Payment, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.bidders = bidders\n",
    "        self.linear = Linear(in_features, bidders)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "class Mechanism(Module):\n",
    "    r\"\"\"Determines the allocation and payment of the bidders for a given input.\n",
    "    \n",
    "    Args:\n",
    "        allocation: the network govering allocation\n",
    "        payment: the network governing payment\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, allocation, payment):\n",
    "        super(Mechanism, self).__init__()\n",
    "        self.allocation = allocation\n",
    "        self.payment = payment\n",
    "        \n",
    "    def forward(self, x):\n",
    "        allocation = self.allocation(x)\n",
    "        return allocation, allocation*self.payment(x)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monotonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "# import from_inputs\n",
    "\n",
    "\n",
    "class _ConstantWeightsAndBiases(nn.Module):\n",
    "    def __init__(self, out_features):\n",
    "        super(_ConstantWeightsAndBiases, self).__init__()\n",
    "        self.weights = Parameter(torch.Tensor(out_features, 1))\n",
    "        self.biases = Parameter(torch.Tensor(out_features, 1))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(self.biases, a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, context):\n",
    "        return self.weights, self.biases\n",
    "    \n",
    "class _VariableWeightsAndBiases(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(_VariableWeightsAndBiases, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, hidden_features)\n",
    "        self.weights = nn.Linear(hidden_features, out_features)\n",
    "        self.biases = nn.Linear(hidden_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear(x))\n",
    "        return self.weights(x), self.biases(x)\n",
    "    \n",
    "class Monotonic(nn.Module):\n",
    "    def __init__(self, hidden_features, linear_functions, groups, context_features=0):\n",
    "        super(Monotonic, self).__init__()\n",
    "        self.context_features = context_features\n",
    "        self.linear_functions = linear_functions\n",
    "        self.groups = groups\n",
    "        \n",
    "        if context_features > 0:\n",
    "            self.weights_and_biases = _VariableWeightsAndBiases(context_features, hidden_features, linear_functions * groups)\n",
    "        else:\n",
    "            self.weights_and_biases = _ConstantWeightsAndBiases(linear_functions * groups)\n",
    "        \n",
    "        \n",
    "    def apply_forward(self, bids, context):\n",
    "        w, b = self.weights_and_biases(context)\n",
    "        intermediate = torch.exp(w) * bids + b\n",
    "        return intermediate.reshape(-1, self.groups, self.linear_functions).max(dim=2)[0].min(dim=1, keepdim=True)[0]\n",
    "        \n",
    "    def apply_inverse(self, bids, vbids, context):\n",
    "        w, b = self.weights_and_biases(context)\n",
    "        intermediate = torch.exp(-w) * (vbids - b)\n",
    "        return intermediate.reshape(-1, self.groups, self.linear_functions).min(dim=2)[0].max(dim=1, keepdim=True)[0]\n",
    "            \n",
    "    def forward(self, inputs, bids=None, invert=False):\n",
    "        x, context = from_inputs(inputs,1)\n",
    "        if invert:\n",
    "            return self.apply_inverse(bids, x, context)\n",
    "        else:\n",
    "            return self.apply_forward(x, context)\n",
    "        \n",
    "def create_monotonic(context_features=0, hidden_features=1, linear_functions=1, groups=1):\n",
    "    return Monotonic(hidden_features, linear_functions, groups, context_features=context_features)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "# from .common import to_inputs\n",
    "# from .common import from_inputs\n",
    "\n",
    "def _dot(alist,blist):\n",
    "    return sum(a*b for a,b in zip(alist,blist))\n",
    "\n",
    "def cascade_outcomes(allocators, pricers, inputs, bidders):\n",
    "    bids, context = from_inputs(inputs, bidders)\n",
    "    allocations, prices = [], []\n",
    "    cumulative_allocation = None\n",
    "    prev_allocatinon_prob = None\n",
    "    \n",
    "    for allocator, pricer in zip(allocators, pricers):\n",
    "        slot_inputs = to_inputs(bids, context)\n",
    "        allocation = allocator(slot_inputs)\n",
    "        price = pricer(slot_inputs)\n",
    "        if cumulative_allocation is None:\n",
    "            allocations.append(allocation)\n",
    "            prices.append(price)\n",
    "            cumulative_allocation = allocation\n",
    "            prev_allocation_prob = torch.cat([allocation.sum(dim=1,keepdim=True) for _ in range(bidders)], dim=1)\n",
    "        else:\n",
    "            unconditional_allocation = allocation * (1-cumulative_allocation)*prev_allocation_prob\n",
    "            allocations.append(unconditional_allocation)\n",
    "            prices.append(price)\n",
    "            cumulative_allocation = cumulative_allocation + unconditional_allocation\n",
    "            prev_allocation_prob = torch.cat([unconditional_allocation.sum(dim=1,keepdim=True) for _ in range(bidders)], dim=1)\n",
    "        bids = (1-cumulative_allocation) * bids\n",
    "    return allocations, prices\n",
    "    \n",
    "class SequentialMechanism(Module):\n",
    "    r\"\"\"Determines the allocation and payment of the bidders for a given input that allows sequential allocation.\n",
    "    \n",
    "    Args:\n",
    "        mechanisms: the networks govering allocation\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, allocators, pricers, bidders, weights=None, virtual_fcn=None):\n",
    "        super(SequentialMechanism, self).__init__()\n",
    "        self.allocators = nn.ModuleList(allocators)\n",
    "        self.pricers = nn.ModuleList(pricers)\n",
    "        self.weights = weights\n",
    "        self.bidders = bidders\n",
    "        self.virtual_fcn = virtual_fcn\n",
    "        \n",
    "    def _compute_virtual_bids(self,bids,context):\n",
    "        return torch.cat(\n",
    "            [self.virtual_fcn(to_inputs(bids[:,i:(i+1)],context)) for i in range(self.bidders)],\n",
    "            dim=1)\n",
    "    \n",
    "    def _compute_prices(self,vprices,bids,context):\n",
    "        return torch.cat(\n",
    "            [self.virtual_fcn(to_inputs(vprices[:,i:(i+1)],context),bids=bids[:,i:(i+1)],invert=True) for i in range(self.bidders)],\n",
    "            dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.virtual_fcn is None:\n",
    "            allocations, prices = cascade_outcomes(self.allocators, self.pricers, x, self.bidders)\n",
    "        else:\n",
    "            bids, context = from_inputs(x, self.bidders)\n",
    "            vbids = self._compute_virtual_bids(bids, context)\n",
    "            vx = to_inputs(vbids,context)\n",
    "            allocations, vprices = cascade_outcomes(self.allocators, self.pricers, vx, self.bidders)\n",
    "            prices = [self._compute_prices(vprices[i], bids, context) for i in range(len(vprices))]\n",
    "            \n",
    "        payments = [a*p for a,p in zip(allocations, prices)]\n",
    "        if self.weights:\n",
    "            return _dot(self.weights, allocations), _dot(self.weights, payments)\n",
    "        else:\n",
    "            return sum(allocations), sum(payments)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Spa\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from . import Mechanism\n",
    "# from . import from_inputs\n",
    "\n",
    "class _SpaAllocation(nn.Module):\n",
    "    def __init__(self, bidders, kappa=1e4):\n",
    "        super(_SpaAllocation, self).__init__()\n",
    "        self.kappa = kappa\n",
    "        self.bidders = bidders\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bids, _ = from_inputs(x, self.bidders)\n",
    "        device = bids.get_device()\n",
    "        bids_plus_dummy = torch.cat([bids, torch.zeros(bids.shape[0],1).to(device)], dim=1)\n",
    "        return torch.softmax(self.kappa*bids_plus_dummy, dim=1)[:,:-1]\n",
    "\n",
    "class _SpaPayment(nn.Module):\n",
    "    def __init__(self, bidders):\n",
    "        super(_SpaPayment, self).__init__()\n",
    "        self.bidders = bidders\n",
    "    def forward(self, x):\n",
    "        bids, _ = from_inputs(x, self.bidders)\n",
    "        device = x.get_device()\n",
    "        return torch.stack([bids.index_fill(1,torch.tensor([col]).to(device),0).max(dim=1)[0] for col in range(bids.shape[1])], dim=1)\n",
    "    \n",
    "def create_spa_allocator(bidders,kappa=1e4):\n",
    "    return _SpaAllocation(bidders,kappa=kappa)\n",
    "\n",
    "def create_spa_pricer(bidders):\n",
    "    return _SpaPayment(bidders,)\n",
    "\n",
    "def create_spa_mechanism(bidders,kappa=1e4):\n",
    "    return Mechanism(\n",
    "        create_spa_allocator(bidders,kappa=1e4),\n",
    "        create_spa_pricer(bidders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidders = 2\n",
    "def create_regret_net(bidders, hidden_layers=2, hidden_units=10):\n",
    "    mbuilder = build_mechanism(bidders)\n",
    "    \n",
    "    # create a layer to correctly size the hidden layers\n",
    "    mbuilder.allocation_builder.add_linear_layer(hidden_units)\n",
    "    mbuilder.payment_builder.add_linear_layer(hidden_units)\n",
    "    \n",
    "    for _ in range(hidden_layers):\n",
    "        mbuilder.allocation_builder.add_linear_layer(hidden_units)\n",
    "        mbuilder.payment_builder.add_linear_layer(hidden_units)\n",
    "        mbuilder.allocation_builder.add_activation(nn.Sigmoid())\n",
    "        mbuilder.payment_builder.add_activation(nn.Sigmoid())\n",
    "    \n",
    "    return mbuilder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "bidders = 2\n",
    "regret_net = create_regret_net(bidders).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/odetteroest/Documents/GitHub/RegretNet example/Pjordan/learning-auctions/alles.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/odetteroest/Documents/GitHub/RegretNet%20example/Pjordan/learning-auctions/alles.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsummary\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/odetteroest/Documents/GitHub/RegretNet%20example/Pjordan/learning-auctions/alles.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m summary(regret_net, (\u001b[39m1\u001b[39;49m,bidders))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[1;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/odetteroest/Documents/GitHub/RegretNet example/Pjordan/learning-auctions/alles.ipynb Cell 17\u001b[0m in \u001b[0;36mMechanism.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/odetteroest/Documents/GitHub/RegretNet%20example/Pjordan/learning-auctions/alles.ipynb#X22sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/odetteroest/Documents/GitHub/RegretNet%20example/Pjordan/learning-auctions/alles.ipynb#X22sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m     allocation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallocation(x)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/odetteroest/Documents/GitHub/RegretNet%20example/Pjordan/learning-auctions/alles.ipynb#X22sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m allocation, allocation\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpayment(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(regret_net, (1,bidders))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1\n",
      "0  0.442670  0.202960\n",
      "1  0.739492  0.011192\n",
      "2  0.145199  0.496675\n",
      "3  0.290939  0.736091\n",
      "4  0.034390  0.690094\n",
      "5  0.228624  0.342884\n",
      "6  0.416825  0.986994\n",
      "7  0.825100  0.447091\n",
      "8  0.021437  0.667179\n",
      "9  0.694569  0.856460\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Om dit te kunnen runnen \n",
    "\n",
    "\n",
    "sample_size = 10 #2**11\n",
    "batch_size = 2 #2**8\n",
    "bidders = 2\n",
    "\n",
    "inputs_loader=data_utils.DataLoader(\n",
    "    data_utils.TensorDataset(torch.rand(sample_size, bidders)),\n",
    "    batch_size=batch_size)\n",
    "data_list = []\n",
    "for batch_idx, batch in enumerate(inputs_loader):\n",
    "    data_list.append(batch[0].numpy())\n",
    "\n",
    "# Concatenate the data and create a DataFrame\n",
    "data = np.concatenate(data_list, axis=0)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "def _calc_utility(inputs, mechanism, values, output_components=False):\n",
    "    allocation, payment = mechanism(inputs)\n",
    "    if output_components:\n",
    "        return allocation*values-payment, allocation, payment\n",
    "    else:\n",
    "        return allocation*values-payment\n",
    "    \n",
    "def _calc_regret(inputs, mechanism, misreport, bidders, device, leaky=False):\n",
    "    values, context = from_inputs(inputs, bidders)\n",
    "    u_true, a_true, p_true = _calc_utility(inputs, mechanism, values, output_components=True)\n",
    "    regret = torch.zeros(values.shape).to(device)\n",
    "    for bidder in range(bidders):\n",
    "        bidder_mask = torch.zeros(values.shape).to(device)\n",
    "        bidder_mask[:,bidder] = 1.0\n",
    "        response = misreport * bidder_mask + values * (1-bidder_mask)\n",
    "        u_response = _calc_utility(to_inputs(response,context), mechanism, values)\n",
    "        if leaky:\n",
    "            regret = regret + F.leaky_relu((u_response - u_true) * bidder_mask)\n",
    "        else:\n",
    "            regret = regret + F.relu((u_response - u_true) * bidder_mask)\n",
    "    return regret.mean(dim=0)\n",
    "\n",
    "def _best_misreport(values, inputs, mechanism, bidders, device, misreport_lr, misreport_epochs):\n",
    "    misreport = (torch.FloatTensor(values.shape).uniform_(0, 1).to(device) * values).detach().requires_grad_(True)\n",
    "    misreport_optimizer = optim.Adam([misreport], lr=misreport_lr)\n",
    "            \n",
    "    mechanism.eval()\n",
    "    for _ in range(misreport_epochs):\n",
    "        misreport_optimizer.zero_grad()\n",
    "        regret = _calc_regret(\n",
    "            inputs, \n",
    "            mechanism,\n",
    "            misreport,\n",
    "            bidders,\n",
    "            device,\n",
    "            leaky=True)\n",
    "        (-regret.sum()).backward()\n",
    "        nn.utils.clip_grad_norm_([misreport], 1.0)\n",
    "        misreport_optimizer.step()\n",
    "    mechanism.train()\n",
    "    return misreport.detach().clone().requires_grad_(False)\n",
    "\n",
    "def train(mechanism, values_loader, bidders, **kwargs):\n",
    "    # load parameters\n",
    "    device = kwargs['device'] if 'device' in kwargs else 'cpu'\n",
    "    epochs = kwargs['epochs'] if 'epochs' in kwargs else 1\n",
    "    rho = kwargs['rho'] if 'rho' in kwargs else 100\n",
    "    mechanism_lr = kwargs['mechanism_lr'] if 'mechanism_lr' in kwargs else 1e-3\n",
    "    misreport_lr = kwargs['misreport_lr'] if 'misreport_lr' in kwargs else 1e-3\n",
    "    misreport_epochs = kwargs['misreport_epochs'] if 'misreport_epochs' in kwargs else 10\n",
    "    consider_dsic = kwargs['consider_dsic'] if 'consider_dsic' in kwargs else True\n",
    "    consider_ir = kwargs['consider_ir'] if 'consider_ir' in kwargs else True\n",
    "    \n",
    "    # Initialize augmented lagrangian parameters\n",
    "    if consider_dsic:\n",
    "        lambda_dsic = torch.zeros(bidders).to(device)\n",
    "    if consider_ir:\n",
    "        lambda_ir   = torch.zeros(bidders).to(device)\n",
    "\n",
    "    # Initalize the optimizer\n",
    "    mechanism_optimizer = optim.Adam(mechanism.parameters(), lr=mechanism_lr)\n",
    "    \n",
    "    report_data = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for batch_idx,(values_list) in enumerate(values_loader):\n",
    "            inputs = values_list[0].to(device)\n",
    "            values, context = from_inputs(inputs, bidders)\n",
    "            print(values)\n",
    "            if consider_dsic:\n",
    "                misreport = _best_misreport(\n",
    "                    values, \n",
    "                    inputs, \n",
    "                    mechanism, \n",
    "                    bidders, \n",
    "                    device, \n",
    "                    misreport_lr, \n",
    "                    misreport_epochs)\n",
    "\n",
    "            # Start the gradient computation\n",
    "            mechanism.zero_grad()\n",
    "            \n",
    "            # calculate the utilities and prices\n",
    "            utility, allocation, payment = _calc_utility(\n",
    "                inputs, mechanism, values, output_components=True)\n",
    "    \n",
    "            if consider_dsic:\n",
    "                # compute expected regret \n",
    "                dsic_violation = _calc_regret(\n",
    "                    inputs,\n",
    "                    mechanism,\n",
    "                    misreport,\n",
    "                    bidders,\n",
    "                    device)\n",
    "            \n",
    "            if consider_ir:\n",
    "                # compute individual rationality violation\n",
    "                ir_violation = F.relu(-utility).mean(dim=0)\n",
    "    \n",
    "            # compute components of the loss function\n",
    "            revenue = payment.sum(dim=-1).mean()\n",
    "            \n",
    "            if consider_dsic:\n",
    "                total_dsic_violation = dsic_violation.sum()\n",
    "            \n",
    "            if consider_ir:\n",
    "                total_ir_violation = ir_violation.sum()\n",
    "            \n",
    "            total_violation = 0\n",
    "            if consider_dsic:\n",
    "                total_violation += total_dsic_violation.pow(2)\n",
    "                \n",
    "            if consider_ir:\n",
    "                total_violation += total_ir_violation.pow(2)\n",
    "                \n",
    "            # define the loss \n",
    "            loss = -revenue+0.5*rho*(total_violation)\n",
    "            \n",
    "            if consider_dsic:\n",
    "                loss += (lambda_dsic*dsic_violation).sum()\n",
    "                \n",
    "            if consider_ir:\n",
    "                loss += (lambda_ir*ir_violation).sum()\n",
    "                \n",
    "            \n",
    "            # Trigger the autogradient calculation\n",
    "            loss.backward()\n",
    "        \n",
    "            # Clip the norm to prevent exploding gradients\n",
    "            nn.utils.clip_grad_norm_(mechanism.parameters(), 1.0)\n",
    "            \n",
    "            # Take a step towards the gradient\n",
    "            mechanism_optimizer.step()\n",
    "            \n",
    "            mechanism.eval()\n",
    "            \n",
    "            if consider_dsic:\n",
    "                misreport = _best_misreport(\n",
    "                    values, \n",
    "                    inputs, \n",
    "                    mechanism, \n",
    "                    bidders, \n",
    "                    device, \n",
    "                    misreport_lr, \n",
    "                    misreport_epochs)\n",
    "            \n",
    "                # Update the augmented lagrangian parameters\n",
    "                dsic_violation_next = _calc_regret(\n",
    "                    inputs,\n",
    "                    mechanism,\n",
    "                    misreport,\n",
    "                    bidders,\n",
    "                    device)\n",
    "                lambda_dsic = (lambda_dsic + rho * dsic_violation_next).detach()\n",
    "            \n",
    "            if consider_ir:\n",
    "                u_next = _calc_utility(inputs, mechanism, values)\n",
    "                ir_violation_next = F.relu(-u_next).mean(dim=0)\n",
    "            \n",
    "                lambda_ir = (lambda_ir + rho * ir_violation_next).detach()\n",
    "            \n",
    "            mechanism.train()\n",
    "            \n",
    "            report_item = {\n",
    "                'epoch': epoch,\n",
    "                'batch': batch_idx,\n",
    "                'revenue':revenue.item(),\n",
    "                'loss':loss.item()}\n",
    "            \n",
    "            if consider_dsic:\n",
    "                report_item['total_dsic_violation']=total_dsic_violation.item()\n",
    "            \n",
    "            if consider_ir:\n",
    "                report_item['total_ir_violation']=total_ir_violation.item()\n",
    "                \n",
    "            report_data.append(report_item)\n",
    "    #print(report_data)    \n",
    "    return report_data\n",
    "\n",
    "\n",
    "def evaluate(mechanism, inputs_loader, bidders, **kwargs):\n",
    "    # load parameters\n",
    "    device = kwargs['device'] if 'device' in kwargs else 'cpu'\n",
    "    misreport_lr = kwargs['misreport_lr'] if 'misreport_lr' in kwargs else 1e-3\n",
    "    misreport_epochs = kwargs['misreport_epochs'] if 'misreport_epochs' in kwargs else 10\n",
    "    consider_dsic = kwargs['consider_dsic'] if 'consider_dsic' in kwargs else True\n",
    "    consider_ir = kwargs['consider_ir'] if 'consider_ir' in kwargs else True\n",
    "    \n",
    "    report_data = []\n",
    "\n",
    "    mechanism.eval()\n",
    "    for batch_idx,(input_list) in enumerate(inputs_loader):\n",
    "        inputs = input_list[0].to(device)\n",
    "        values, context = from_inputs(inputs, bidders)\n",
    "        \n",
    "        if consider_dsic:\n",
    "            misreport = _best_misreport(\n",
    "                values, \n",
    "                inputs, \n",
    "                mechanism, \n",
    "                bidders, \n",
    "                device, \n",
    "                misreport_lr, \n",
    "                misreport_epochs)\n",
    "            \n",
    "        # calculate the utilities and prices\n",
    "        utility, allocation, payment = _calc_utility(\n",
    "            inputs, mechanism, values, output_components=True)\n",
    "    \n",
    "        if consider_dsic:\n",
    "            # compute expected regret \n",
    "            dsic_violation = _calc_regret(\n",
    "                inputs,\n",
    "                mechanism,\n",
    "                misreport,\n",
    "                bidders,\n",
    "                device)\n",
    "    \n",
    "        if consider_ir:\n",
    "            # compute individual rationality violation\n",
    "            ir_violation = F.relu(-utility).mean(dim=0)\n",
    "    \n",
    "        # compute components of the loss function\n",
    "        revenue = payment.sum(dim=-1).mean()\n",
    "        \n",
    "        if consider_dsic:\n",
    "            total_dsic_violation = dsic_violation.sum()\n",
    "        \n",
    "        if consider_ir:\n",
    "            total_ir_violation = ir_violation.sum()\n",
    "        \n",
    "        report_item = {\n",
    "            'batch': batch_idx,\n",
    "            'revenue':revenue.item(),\n",
    "        }\n",
    "        \n",
    "        if consider_dsic:\n",
    "            report_item['total_dsic_violation']=total_dsic_violation.item()\n",
    "            \n",
    "        if consider_ir:\n",
    "            report_item['total_ir_violation']=total_ir_violation.item()\n",
    "            \n",
    "        report_data.append(report_item)\n",
    "        \n",
    "    mechanism.train()        \n",
    "    return report_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4427, 0.2030],\n",
      "        [0.7395, 0.0112]])\n",
      "tensor([[0.1452, 0.4967],\n",
      "        [0.2909, 0.7361]])\n",
      "tensor([[0.0344, 0.6901],\n",
      "        [0.2286, 0.3429]])\n",
      "tensor([[0.4168, 0.9870],\n",
      "        [0.8251, 0.4471]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:02,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0214, 0.6672],\n",
      "        [0.6946, 0.8565]])\n",
      "tensor([[0.4427, 0.2030],\n",
      "        [0.7395, 0.0112]])\n",
      "tensor([[0.1452, 0.4967],\n",
      "        [0.2909, 0.7361]])\n",
      "tensor([[0.0344, 0.6901],\n",
      "        [0.2286, 0.3429]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:02,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4168, 0.9870],\n",
      "        [0.8251, 0.4471]])\n",
      "tensor([[0.0214, 0.6672],\n",
      "        [0.6946, 0.8565]])\n",
      "tensor([[0.4427, 0.2030],\n",
      "        [0.7395, 0.0112]])\n",
      "tensor([[0.1452, 0.4967],\n",
      "        [0.2909, 0.7361]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:01,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0344, 0.6901],\n",
      "        [0.2286, 0.3429]])\n",
      "tensor([[0.4168, 0.9870],\n",
      "        [0.8251, 0.4471]])\n",
      "tensor([[0.0214, 0.6672],\n",
      "        [0.6946, 0.8565]])\n",
      "tensor([[0.4427, 0.2030],\n",
      "        [0.7395, 0.0112]])\n",
      "tensor([[0.1452, 0.4967],\n",
      "        [0.2909, 0.7361]])\n",
      "tensor([[0.0344, 0.6901],\n",
      "        [0.2286, 0.3429]])\n",
      "tensor([[0.4168, 0.9870],\n",
      "        [0.8251, 0.4471]])\n",
      "tensor([[0.0214, 0.6672],\n",
      "        [0.6946, 0.8565]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:01<00:01,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4427, 0.2030],\n",
      "        [0.7395, 0.0112]])\n",
      "tensor([[0.1452, 0.4967],\n",
      "        [0.2909, 0.7361]])\n",
      "tensor([[0.0344, 0.6901],\n",
      "        [0.2286, 0.3429]])\n",
      "tensor([[0.4168, 0.9870],\n",
      "        [0.8251, 0.4471]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:01<00:01,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0214, 0.6672],\n",
      "        [0.6946, 0.8565]])\n",
      "tensor([[0.4427, 0.2030],\n",
      "        [0.7395, 0.0112]])\n",
      "tensor([[0.1452, 0.4967],\n",
      "        [0.2909, 0.7361]])\n",
      "tensor([[0.0344, 0.6901],\n",
      "        [0.2286, 0.3429]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:01<00:01,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4168, 0.9870],\n",
      "        [0.8251, 0.4471]])\n",
      "tensor([[0.0214, 0.6672],\n",
      "        [0.6946, 0.8565]])\n",
      "tensor([[0.4427, 0.2030],\n",
      "        [0.7395, 0.0112]])\n",
      "tensor([[0.1452, 0.4967],\n",
      "        [0.2909, 0.7361]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:01<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0344, 0.6901],\n",
      "        [0.2286, 0.3429]])\n",
      "tensor([[0.4168, 0.9870],\n",
      "        [0.8251, 0.4471]])\n",
      "tensor([[0.0214, 0.6672],\n",
      "        [0.6946, 0.8565]])\n",
      "tensor([[0.4427, 0.2030],\n",
      "        [0.7395, 0.0112]])\n",
      "tensor([[0.1452, 0.4967],\n",
      "        [0.2909, 0.7361]])\n",
      "tensor([[0.0344, 0.6901],\n",
      "        [0.2286, 0.3429]])\n",
      "tensor([[0.4168, 0.9870],\n",
      "        [0.8251, 0.4471]])\n",
      "tensor([[0.0214, 0.6672],\n",
      "        [0.6946, 0.8565]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:02<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4427, 0.2030],\n",
      "        [0.7395, 0.0112]])\n",
      "tensor([[0.1452, 0.4967],\n",
      "        [0.2909, 0.7361]])\n",
      "tensor([[0.0344, 0.6901],\n",
      "        [0.2286, 0.3429]])\n",
      "tensor([[0.4168, 0.9870],\n",
      "        [0.8251, 0.4471]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:02<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0214, 0.6672],\n",
      "        [0.6946, 0.8565]])\n",
      "tensor([[0.4427, 0.2030],\n",
      "        [0.7395, 0.0112]])\n",
      "tensor([[0.1452, 0.4967],\n",
      "        [0.2909, 0.7361]])\n",
      "tensor([[0.0344, 0.6901],\n",
      "        [0.2286, 0.3429]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4168, 0.9870],\n",
      "        [0.8251, 0.4471]])\n",
      "tensor([[0.0214, 0.6672],\n",
      "        [0.6946, 0.8565]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "report = pd.DataFrame(train(\n",
    "    regret_net, # the mechanism\n",
    "    inputs_loader, # the bid inputs\n",
    "    bidders, # the number of bidders\n",
    "    epochs=10, # the total number of loops over the data\n",
    "    device=device, # the device\n",
    "    rho=1e2, # the rho parameter for the augmented Lagrangian method\n",
    "    mechanism_lr=1e-2, # the learning rate for the mechanism networks\n",
    "    misreport_lr=1e-1, # the learning rate for the misreport tensor\n",
    "    misreport_epochs=10)) # the number of epochs to tune the misreport tensor for each batch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
