{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear\n",
    "\n",
    "def _sequential_allocation(p, weights):\n",
    "    _, slots, bidders_plus_one = p.shape\n",
    "    bidders = bidders_plus_one - 1\n",
    "    \n",
    "    # total probability of allocating to slot 0\n",
    "    cumulative_total = p[:,0,:bidders]\n",
    "    \n",
    "    # weighted total allocation\n",
    "    if weights is None:\n",
    "        alloc = cumulative_total\n",
    "    else:\n",
    "        alloc = cumulative_total * weights[0]\n",
    "    \n",
    "    for k in range(1,slots):\n",
    "        # total probability of allocating to slot k\n",
    "        slot_total = (1-cumulative_total)*p[:,k,:bidders]*(1-p[:,k-1,[bidders for _ in range(bidders)]])\n",
    "\n",
    "        # weighted total allocation\n",
    "        if weights is None:\n",
    "            alloc = alloc + slot_total\n",
    "        else:\n",
    "            alloc = alloc + slot_total * weights[k]\n",
    "        \n",
    "        cumulative_total = cumulative_total + slot_total\n",
    "    return alloc\n",
    "    \n",
    "class Allocation(Module):\n",
    "    r\"\"\"Determines allocation probability for each of the bidders given an input.\n",
    "    \n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        bidders: number of bidders, which governs the size of each output sample\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
    "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
    "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{bidders}`.\n",
    "\n",
    "    Examples::\n",
    "        >>> m = Allocation(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> allocation = m(input)\n",
    "        >>> print(allocation.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "    __constants__ = ['in_features', 'bidders']\n",
    "    \n",
    "    def __init__(self, in_features, bidders):\n",
    "        super(Allocation, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.bidders = bidders\n",
    "        self.linear = Linear(in_features, bidders+1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.linear(x), dim=1)[:, 0:self.bidders]\n",
    "\n",
    "class SequentialAllocation(Module):\n",
    "    __constants__ = ['in_features', 'bidders', 'slots', 'weights']\n",
    "    \n",
    "    def __init__(self, in_features, slots, bidders, weights=None):\n",
    "        super(SequentialAllocation, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.slots = slots\n",
    "        self.bidders = bidders\n",
    "        self.weights = weights\n",
    "        self.linear = Linear(in_features, slots * (bidders+1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        probs = F.softmax(self.linear(x).reshape(-1, self.slots, self.bidders+1), dim=2)\n",
    "        return _sequential_allocation(probs,weights=self.weights)\n",
    "\n",
    "class Payment(Module):\n",
    "    r\"\"\"Determines the contingent payment for each of the bidders given an input.\n",
    "    \n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        bidders: number of bidders, which governs the size of each output sample\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
    "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
    "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{bidders}`.\n",
    "\n",
    "    Examples::\n",
    "        >>> m = Allocation(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> payment = m(input)\n",
    "        >>> print(payment.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, bidders):\n",
    "        super(Payment, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.bidders = bidders\n",
    "        self.linear = Linear(in_features, bidders)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "class Mechanism(Module):\n",
    "    r\"\"\"Determines the allocation and payment of the bidders for a given input.\n",
    "    \n",
    "    Args:\n",
    "        allocation: the network govering allocation\n",
    "        payment: the network governing payment\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, allocation, payment):\n",
    "        super(Mechanism, self).__init__()\n",
    "        self.allocation = allocation\n",
    "        self.payment = payment\n",
    "        \n",
    "    def forward(self, x):\n",
    "        allocation = self.allocation(x)\n",
    "        return allocation, allocation*self.payment(x)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
